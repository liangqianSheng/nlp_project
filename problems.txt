代码的内容大概分为以下几个部分：
1. 导入预先训练好的词向量文件整理成字典的形式方便后续调用
2. 导入新闻的csv文件
3. 利用第二课学习的内容计算每个词的词频，存在了word_frequency这个txt里面，后续直接从里面拿，就不用再重新计算了
4. split_sentences_2此函数作用是对整段文字进行句子分割
5. SIF——sentence——embedding函数是用到了指导书里面提到的sif方法，pca降维有需要可以加上，感觉不加也行。
6. knn是后续做平滑处理用的函数
7. get——corr函数计算每个句子的分数，分数包括了：题目的相似程度和整段文字的相似程度加权以及knn平滑，这里的一些参数调参的同学可以随意尝试
8. 最后get——summarization两个函数就是综合了一下前面的，输出摘要，摘要的长度也是可以修改的。

根据目前情况，发现以下问题：
1. 一些转折词，连词，比如“相反，报道称”这种词的得分总是很高，有时候会对摘要的语句通顺产生一些影响。
2. 开头第一句话往往比较重要，所以我给第一句话加了一些分数，这个和位置有关的句子重要性后续大家有啥好的想法也可以一起讨论讨论。
3. 因为句子按照逗号分开计算，排序结束后句子里面某些部分会因为得分低而被删除，所以句子有时候会有点奇怪，但有时候确实可以删去多余的部分，这个地方还是需要提升的。
4. 后续有时间可以试试别的句子重要性排序方法，组合一下看看效果会不会好一些。
